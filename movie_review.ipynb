{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "#### First load the dataset, download at http://www.cs.cornell.edu/People/pabo/movie-review-data/, open each file, read all its content as a string, delete punctuations and transfer it into a word list, then map each word to an integer index according to the word index dictionary downloaded at https://s3.amazonaws.com/text-datasets/imdb_word_index.json, only consider 5000 most frequently used words.\n",
    "#### Next, create an embedding layer, propagate training data through it to get word embeddings(every 32 dimensional vector represents a word).\n",
    "#### Then use Keras to create a LSTM layer with 100 units to get the output, use dense layer and sigmoid activation function to get the final result, if>0.5, it is positive, else negative. Use adam optimizer to optimize the layer.\n",
    "#### Then check its training and testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "import json\n",
    "from keras.utils import get_file\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index(path='imdb_word_index.json'):\n",
    "    path = get_file(path,origin='https://s3.amazonaws.com/text-datasets/imdb_word_index.json',\n",
    "                    file_hash='bfafd718b763782e994055a2d397834f')\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=get_word_index(path='imdb_word_index.json')\n",
    "path = \"Desktop/review_polarity/txt_sentoken/neg\" \n",
    "files= os.listdir(path)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "X_neg = []\n",
    "for file in files: \n",
    "          f = open(path+\"/\"+file)\n",
    "          iter_f = iter(f)\n",
    "          str = \"\"\n",
    "          for line in iter_f: \n",
    "              str = str + line.lower()\n",
    "          temp=tokenizer.tokenize(str)\n",
    "          X_neg.append(temp)\n",
    "X_neg=[[word_index[w] for w in x if w in word_index.keys() and word_index[w]<5000]for x in X_neg]\n",
    "Y_neg=[0 for w in range(len(X_neg))]\n",
    "path2 = \"Desktop/review_polarity/txt_sentoken/pos\" \n",
    "files2= os.listdir(path2) \n",
    "X_pos = []\n",
    "for file2 in files2: \n",
    "          f = open(path2+\"/\"+file2)\n",
    "          iter_f = iter(f)\n",
    "          str = \"\"\n",
    "          for line in iter_f: \n",
    "              str = str + line.lower()\n",
    "          temp=tokenizer.tokenize(str)\n",
    "          X_pos.append(temp)\n",
    "X_pos=[[word_index[w] for w in x if w in word_index.keys()and word_index[w]<5000]for x in X_pos]\n",
    "Y_pos=[1 for w in range(len(X_pos))]\n",
    "X=X_neg+X_pos\n",
    "Y=Y_neg+Y_pos\n",
    "np.random.seed(10)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "x=[]\n",
    "y=[]\n",
    "for i in indices:\n",
    "    x.append(X[i])\n",
    "    y.append(Y[i])    \n",
    "idx = int(0.8*len(X))\n",
    "X_train, y_train = np.array(x[:idx]), np.array(y[:idx])\n",
    "X_test, y_test = np.array(x[idx:]), np.array(y[idx:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad X_train and X_test to same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 1000\n",
    "X_train=sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
    "X_test=sequence.pad_sequences(X_test, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model with embedding and LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(top_words,embedding_vector_length,input_length=max_review_length))\n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(\"Accuracy: %.2f%%\"%(scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
